---
title: "Akshaya_Final_Assignment"
output:
  pdf_document: default
  html_document: default
date: "2022-08-15"
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 - Probability Practice:

### Part A:

P(Y) = 0.65 & P(N) = 0.35 (Y = Yes, N = No)

P(R) = 0.3 & P(T) = 0.7 (R = Random Clicker, T = Truthful Clicker)

P(Y\|R) = 0.5 = P(N\|R)

P(Y) = P(R) \* P(Y\|R) + P(T) \* P(Y\|T)

=\> P(Y\|T) = (P(Y) - P(R) \* P(Y\|R))/P(T)

Substituting the values we get:

P(Y\|T) = (0.65 - 0.3 \* 0.5)/0.7

**P(Y\|T) = 0.7143** or **71.43%**

### Part B:

P(P\|D) = 0.993, P(N\|no D) = 0.9999 (P = positive test, N = negative test, D = have disease, no D = have no disease)

P(D) = 0.000025 =\> P(no D) = 0.999975

P(D\|P) = P(D,P)/P(P) = **P(P\|D) \* P(D)/P(P)**

In the highlighted expression we know P(P\|D), P(D). For P(P):

P(P) = P(P\|D) \* P(D) + P(P\|no D) \* P(no D)

=\> P(P) = P(P\|D) \* P(D) + P(P,no D)

=\> P(P) = P(P\|D) \* P(D) + P(no D) - P(N,no D)

=\> **P(P) = P(P\|D) \* P(D) + P(no D) - P(no D) \* P(N\|no D)**

In the highlighted equation we know all the terms on the right

P(P) = 0.993 \* 0.000025 + 0.999975 - 0.999975 \* 0.9999 = 0.0001248225

Now:

P(D\|P) = 0.993 \* 0.000025/0.0001248225

**P(D\|P) = 0.1989** or **19.89%**

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

## Question 2 - Wrangling the Billboard Top 100

### Part A:

```{r}
library(dplyr)
library(tidyverse)
data1 <- read.csv("billboard.csv")
data2<-data1[,c("performer","song","year","week","week_position")]

data3<-data2%>%group_by(performer,song)%>%summarize(count = n())%>%arrange(desc(count)) 
print('THe sorted dataframe is below')
head(data3,n=10)
```

### Part B:

```{r}
library(ggplot2)
data4<-unique(data2[,c("year","song")])
data5<-data4%>%group_by(year)%>%summarize(count = n())

data5<-data5 %>% filter(year !=1958 & year !=2021)
print('Plotting the data required')
plot(data5)
```

### Part C:

```{r}

data6<-unique(data2[,c("performer","song","week")])
data7<-data6%>%group_by(performer,song)%>%summarize(count = n())

data8<-data7%>% filter(count >=10)
data9<-unique(data8[,c("performer","song")])
data10<-data9%>%group_by(performer)%>%summarize(count = n())
data11<-data10%>% filter(count >30)

ggplot(data11) + 
  geom_col(aes(x=performer, y=count)) +coord_flip()

print('Elton John seems to have the most 10 week hits in the billboard top 100.')

```

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

##Question 3 - Visual story telling part 1: green buildings

```{r}
gb_data = read.csv('/Users/akshayamudar/Desktop/greenbuildings.csv')
head(gb_data)

gb_data %>%
  ggplot(aes(x = leasing_rate)) + geom_histogram() + labs(x = "Occupancy", y = "No. of buildings")
```

So, the greenbuildings.csv data has been read in. As seen in the histogram for Occupancy above, we have some buildings which have very low occupancy (\< 10%). So as done by the developer's on-staff, the buildings with occupancy less than 10% will be removed.

```{r}
gb_data_clean = gb_data %>%
  filter(leasing_rate >= 10)

cat('The no.of buildings with Occupancy less than 10% is:',nrow(gb_data) - nrow(gb_data_clean),'\n')

low_occupancy_green = gb_data %>%
  filter(leasing_rate < 10 & green_rating == 1) %>%
  nrow()
cat('The no.of buildings with Occupancy less than 10% and have green rating:',low_occupancy_green)
```

215 buildings have been removed from the dataset due to very low occupancy. Also, only one of them have green rating. As seen in the above and as done by the developer's on-staff, the buildings with occupancy less than 10% have been removed.

```{r}
gb_data_clean["green_rating_factor"] = as.factor(gb_data_clean[,"green_rating"])
ggplot(gb_data_clean, aes(x=green_rating_factor, y=Rent)) + 
    geom_boxplot()
```

From he above graph, it is very clear that the buildings with green rating have a slightly higher rent compared to those that do not have green rating

```{r}
med_gb = gb_data_clean %>%
  group_by(green_rating) %>%
  summarize(median_rent = median(Rent))
med_gb
```

The median values for the green buildings and non-green buildings also seem to be correct. But we need to confirm if the rent is mostly affected by the green rating and no other factors.

```{r}
gb_cor_data = gb_data[,c(5,3,4,6,7,8,9,10,11,14,15,16,17,18,19,20,21,22,23)]
ggcorrplot::ggcorrplot(cor(gb_cor_data))
```

From the correlation plot, we can see that *Rent* is highly correlated with *cluster_rent*. This means that the rent of a building depends on the area in which the building is located. So we need to see if, in each cluster the green building has the highest or atleast top 3 in terms of rent.

```{r}
gb_max_rent = gb_data_clean %>%
  group_by(cluster) %>%
  top_n(n = 3, wt = Rent) %>%
  group_by(cluster) %>%
  summarize(green_present = sum(green_rating))
  
gb_max_rent[1:10,]
```

We created a table of cluster number and a column called *green_present* which has two inputs i.e., **0** and **1**. 1 implies that the green building present in the corresponding cluster is in the top 3 in terms of the rent. O implies that the green building is not in the top 3.

```{r}
gb_max_rent %>%
  group_by(green_present) %>%
  summarize(count = n()) %>%
  mutate(green_present = if_else(green_present == 1,"Green building in top 3","Green building not in top 3")) %>%
  ggplot(aes(green_present,count)) + geom_bar(stat = "identity") +ylab("No. of clusters")
```

In maximum number of clusters green building is in the top 3 buildings in terms of rent. But there is a considerable number of clusters in which the green building is not even in the top 3. Let us see in how many clusters we have the green building actually at the top.

```{r}
gb_data_clean %>%
  group_by(cluster) %>%
  top_n(n = 1, wt = Rent) %>%
  group_by(cluster) %>%
  summarize(green_top = sum(green_rating)) %>%
  group_by(green_top) %>%
  summarize(count = n()) %>%
  mutate(green_top = if_else(green_top == 1,"Green building","Non - Green building")) %>%
  ggplot(aes(green_top,count)) + geom_bar(stat = "identity") +ylab("No. of clusters")
```

As we can see, more clusters have a non-green building with the highest rent. This confirms that having a green rating does not necessarily lead to higher rents. Also, we can see that *age*, *class-a*, *class-b* and *renovated* columns have a possibility of confounding relationships with *Rent* and *green_rating*.

```{r}
gb_data_plots = gb_data[,c(5,8,9,10,11,14)]
gb_data_plots[,"green_rating"] = as.factor(gb_data_plots[,"green_rating"])
gb_data_plots[,"class_a"] = as.factor(gb_data_plots[,"class_a"])
gb_data_plots[,"class_b"] = as.factor(gb_data_plots[,"class_b"])
gb_data_plots[,"renovated"] = as.factor(gb_data_plots[,"renovated"])

ggplot(gb_data_plots,aes(age,Rent)) + geom_point()
ggplot(gb_data_plots,aes(green_rating,age)) + geom_boxplot()

ggplot(gb_data_plots,aes(renovated,Rent)) + geom_boxplot()
table(gb_data_plots[,c(3,6)])

ggplot(gb_data_plots,aes(class_a,Rent)) + geom_boxplot()
table(gb_data_plots[,c(4,6)])

ggplot(gb_data_plots,aes(class_b,Rent)) + geom_boxplot()
table(gb_data_plots[,c(5,6)])
```

We can see from the graphs that *green_rating* and *Rent* have similar correlations with *age*, *class-a*, *class-b* and *renovated*.

-   As *age* increases *Rent* goes down. In the box plot between *green_rating* and *age*, *green_rating* **0** has higher median *age*.

-   From the confusion matrix between *renovated* and *green_rating* we see that the probability of having a *green_rating* 1 goes down from **0.08** to **0.05** given the condition that *renovated* is **1**. This implies that if a building has undergone substantial renovations in its lifetime then it is far from having a green rating. That is again connected to the age of the building. Older buildings are usually renovated

-   *Class-a* buildings have higher rents. The condition that the given building is *class-a* rated increases the probability of it being a green-rated building from **0.08** to **0.173**

-   *Class-b* buildings have lower rents. The condition that the given building is *class-b* rated decreases the probability of it being a green-rated building from **0.08** to **0.03**.

Hence, we can conclude that there are confounding variables which might give the impression that green-rating leads to higher rents.

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

## Question 4 - Visual story telling part 2: Capital Metro data

```{r}
metro_data = read.csv('/Users/akshayamudar/Desktop/capmetro_UT.csv')
head(metro_data)
metro_data['timestamp'] <- as.POSIXct(metro_data[,'timestamp'], format = "%Y-%m-%d %H:%M:%S")
metro_data['Time'] = format(metro_data[,'timestamp'], format = "%H:%M:%S")
metro_data['Date'] = format(metro_data[,'timestamp'], format = "%Y-%m-%d")
```

```{r}
library(dplyr)
library(ggplot2)
metro_data %>%
  group_by(Time,month) %>%
  summarize(mean_temp = mean(temperature)) %>%
  ggplot(aes(Time,mean_temp, color = month)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90)) + scale_color_hue(direction = -1)
```

The above scatter plot is between time of the day and mean temperature at that time of the day in a particular month. We can clearly see that in all the three months the temperature starts at lowest point at 6 in the morning, rises to a peak in the afternoon and cools down throughout the evening and night. Also, we can see that Sep is hotter than October and October is hotter than November. This represents the arrival of winter season in November.

```{r}
metro_data %>%
  group_by(hour_of_day,Date,month) %>%
  summarize(boarding = mean(boarding)) %>%
  group_by(hour_of_day,month) %>%
  summarize(mean_boarding = mean(boarding)) %>%
  ggplot(aes(hour_of_day,mean_boarding)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90))+facet_wrap(~fct_rev(month), dir = "v")

metro_data %>%
  group_by(hour_of_day,Date,month) %>%
  summarize(alighting = mean(alighting)) %>%
  group_by(hour_of_day,month) %>%
  summarize(mean_alighting = mean(alighting)) %>%
  ggplot(aes(hour_of_day,mean_alighting)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90))+facet_wrap(~fct_rev(month), dir = "v")
```

-   Most of the boarding of the Capital Metro buses at the UT campus is happening in the evening hours when classes are over for most of the students and they are leaving from the campus

-   Most of the alighting of the Capital Metro buses at the UT campus is happening in the morning hours when classes are going to start for most of the students and they are coming to the campus

We have the data from September to November. In all the three months we see similar trend for alighting and boarding. But, these trends might not be repeated in December because of the winter holidays which will lead to lower student population around the campus. But we do have weekends data which can represent December month.

```{r}
metro_data %>%
  group_by(hour_of_day,weekend) %>%
  summarize(mean_boarding = mean(boarding)) %>%
  ggplot(aes(hour_of_day,mean_boarding)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90))+facet_wrap(~fct_rev(weekend), dir = "v")

metro_data %>%
  group_by(hour_of_day,weekend) %>%
  summarize(mean_alighting = mean(alighting)) %>%
  ggplot(aes(hour_of_day,mean_alighting)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90))+facet_wrap(~fct_rev(weekend), dir = "v")
```

We can see that during the weekends there is not much activity as there are no classes at UT on weekends.

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

## Question 5 - Portfolio modeling

In this exercise I am going to take three different portfolios. The first portfolio will have 5 ETFs which are considered to be very safe. In the second portfolio, I am going to consider 5 ETFs which are considered to be highly risky. And in the third I am going to take 3 safe and 2 risky ETFs. In all the portfolios, we are starting with an initial capital of \$100,000 and checking the final return at the end of 20 days.

### Portfolio 1:

The ETFs considered are *SPDR Bloomberg 1-3 Month T-Bill (BIL)*, *iShares Short Treasury Bond (SHV)*, *Invesco Ultra Short Duration (GSY)*, *Goldman Sachs Access Treasury 0-1 Year (GBIL)* and *SPDR SSGA Ultra Short Term Bond (ULST)*. These ETFs are considered to be very safe and not much variation is expected in terms of returns.

```{r}
library(mosaic)
library(quantmod)
library(foreach)

safestocks = c("BIL", "SHV", "GSY", "GBIL", "ULST")
prices_safe = getSymbols(safestocks, from = "2017-01-01")

for(ticker in safestocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

all_returns_safe = cbind(	ClCl(BILa),
								ClCl(SHVa),
								ClCl(GSYa),
								ClCl(GBILa),
								ClCl(ULSTa))
all_returns_safe = as.matrix(na.omit(all_returns_safe))

head(all_returns_safe)
```

As we can see, the change in the closing prices of these ETFs is very low. Let's use Bootstrap resampling and see the variation in the next 20 days.

```{r}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_safe, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

hist(sim1[,n_days], 25, xlab = "Value after 20 days")
cat("The mean value we have at the end of 20 days is ",mean(sim1[,n_days]),"\n")

hist(sim1[,n_days]- initial_wealth, breaks=30, xlab = "Net Return after 20 days")
cat("The mean total variation is ", mean(sim1[,n_days] - initial_wealth), "\n")

cat("VaR is ",quantile(sim1[,n_days]- initial_wealth, prob=0.05)*-1)
```

As we were expecting, the variation in returns for this portfolio is very low and hence we get a low VaR. Also, the histogram for the returns has a low standard deviation which also corroborates to the fact that the ETFs in this portfolio are safe.

### Portfolio 2:

The ETFs considered are *ProShares UltraPro QQQ (TQQQ)*, *ProShares Ultra QQQ (QLD)*, *Direxion Daily S&P 500 Bull 3x Shares (SPXL)*, *Direxion Daily S&P 500 Bull 2x Shares (SPUU)* and *Direxion Daily 20+ Year Treasury Bull 3x Shares (TMF)*. These ETFs are considered to be highly volatile.

```{r}
riskystocks = c("TQQQ", "QLD", "SPXL", "SPUU", "TMF")
prices_risky = getSymbols(riskystocks, from = "2017-01-01")

for(ticker in riskystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

all_returns_risky = cbind(	ClCl(TQQQa),
								ClCl(QLDa),
								ClCl(SPXLa),
								ClCl(SPUUa),
								ClCl(TMFa))
all_returns_risky = as.matrix(na.omit(all_returns_risky))

head(all_returns_risky)
```

We can see that the daily variation of these ETFs are high compared to the Portfolio 1 ETFs that we have seen before.

```{r}
initial_wealth = 100000
sim2 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_risky, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

hist(sim2[,n_days], 25, xlab = "Value after 20 days")
cat("The mean value we have at the end of 20 days is ",mean(sim2[,n_days]),"\n")

hist(sim2[,n_days]- initial_wealth, breaks=30, xlab = "Net Return after 20 days")
cat("The mean total variation is ", mean(sim2[,n_days] - initial_wealth), "\n")

cat("VaR is ",quantile(sim2[,n_days]- initial_wealth, prob=0.05)*-1)
```

The variation in returns for this portfolio is high and hence we get a high VaR. Also, the histogram for the returns has a high standard deviation which also corroborates to the fact that the ETFs in this portfolio are volatile.

### Portfolio 3:

The ETFs considered are *ProShares UltraPro QQQ (TQQQ)*, *ProShares Ultra QQQ (QLD)*, *SPDR Bloomberg 1-3 Month T-Bill (BIL)*, *iShares Short Treasury Bond (SHV)*, *Invesco Ultra Short Duration (GSY)*. We have 2 volatile and 3 safe ETFs in this portfolio.

```{r}
all_returns_mixed = cbind(	ClCl(TQQQa),
								ClCl(QLDa),
								ClCl(BILa),
								ClCl(SHVa),
								ClCl(GSYa))
all_returns_mixed = as.matrix(na.omit(all_returns_mixed))

head(all_returns_mixed)
```

We see that the variation in the closing prices for the two risky portfolios are in another scale compared to the three safe portfolios.

```{r}
initial_wealth = 100000
sim3 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_mixed, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

hist(sim3[,n_days], 25, xlab = "Value after 20 days")
cat("The mean value we have at the end of 20 days is ",mean(sim3[,n_days]),"\n")

hist(sim3[,n_days]- initial_wealth, breaks=30, xlab = "Net Return after 20 days")
cat("The mean total variation is ", mean(sim3[,n_days] - initial_wealth), "\n")

cat("VaR is ",quantile(sim3[,n_days]- initial_wealth, prob=0.05)*-1)
```

For this mixed portfolio, we can see that the VaR is lower compared to the *Portfolio 2* which had all the risky ETFs. We can also see that the histogram for the final return has more spread compared to the one in *Portfolio 1* but lower spread compared to the one in *Portfolio 2*.

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

## Question 6 - Clustering and PCA

```{r}
library(ggplot2)
library(foreach)
library(mosaic)
wine_data = read.csv('/Users/akshayamudar/Desktop/wine.csv')
head(wine_data)
cor_data = wine_data
cor_data[,13] = ifelse(cor_data[,13] == 'red',1,0)
ggcorrplot::ggcorrplot(cor(cor_data), hc.order = TRUE)
```

The *color* column has been modified to binary representation. If the wine is red it will be **1** else **0**. Now, since all the columns are numerical a correlation plot has been created to see the correlation between the columns of interest (*color* and *quality*). We notice for *color* column has high positive correlation with *volatile.acidity* and *fixed.acidity*.

```{r}
X = wine_data[,-c(12,13)]
X_scaled = scale(X, center=TRUE, scale=TRUE)

mu = attr(X_scaled,"scaled:center")
sigma = attr(X_scaled,"scaled:scale")

cluster_color = kmeans(X_scaled, 2, nstart=25)
qplot(fixed.acidity, volatile.acidity, data=wine_data, color=factor(cluster_color$cluster))
qplot(fixed.acidity, volatile.acidity, data=wine_data, color= color)
```

The first scatter plot is between *fixed.acidity* and *volatile.acidity* columns from the Wine data. These two columns are selected because they both have a positive correlation with color column. Which means for higher values, the wine is going to be red wine. The colors are based on the clusters each data point is assigned to using K-means clustering. The second plot is between the same attributes but the colors are based on whether the datapoint corresponds to a red wine or a white wine. When we compare the two plots we can see that with a few exceptions, the datapoints have been clustered correctly.

```{r}
cluster_quality = kmeans(X_scaled, 7, nstart=25)
qplot(pH, alcohol, data=wine_data, color=factor(cluster_quality$cluster))
qplot(pH, alcohol, data=wine_data, color= factor(quality))
```

The plots are between *pH* and *alcohol* but in the first plot, the color coding is based on the clusters each of the data points have been assigned to. The color coding the second plot is based on the quality rating of the wine. From the plots we can see that the k-means clustering did not do a good job in clustering the wines to correct qualities. We will use PCA (Principal Components Analysis) to see what components come up.

```{r}
pca = prcomp(X_scaled, scale=TRUE, rank=1)
summary(pca)
print('Coefficients of x variables to get Principal Component 1')
pca$rotation[,1]

qplot(fixed.acidity, volatile.acidity, data=wine_data, color= color)
qplot(fixed.acidity, volatile.acidity, data=wine_data, color=pca$x[,1])+scale_color_gradient(low = 'blue', high='red')
```

We see that from the co-efficents of x variables for PC1 it seems like if it is positive it is mostly white wine else it is a red wine. This can be interpreted by comparing the coefficient values and the correlation between color and other features. For example, the coefficient for *total.sulfur.dioxide* is positive and the highest in magnitude. If we see in the correlation matrix plot for *color* and *total.sulfur.dioxide* has a very negative correlation. The direction is opposite as PC1 is measuring the whiteness of the wine whereas the color column used for the correlation matrix is taking red wine as 1. A scatter plot between fixed.acidity and volatile.acidity has been created twice. Once with the color of the points determined by the type of wine column (red or white). In the second graph, the color of the points is determined by the value of PC1. The type of wine is clearly visible in the plot with the PC1 based gradient coloring.

```{r}
qplot(pH, alcohol, data=wine_data, color= factor(quality))
qplot(pH, alcohol, data=wine_data, color=pca$x[,1])+scale_color_gradientn(colors = rainbow(7))
```

We again plotted two scatter plots but this time it is between *pH* and *alcohol*. The first graph is color coded by the quality rating of the wine. The second graph is gradient color coded based on PC1 value. Compared to K-means clustering the quality of wines is more accurately divided by PCA with one Principal Component.

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

## Question 7 - Market segmentation

```{r}
library(DataExplorer)
social_dt <- read.csv("/Users/akshayamudar/Desktop/social_marketing.csv")

print('Filter dataset on spam and Adult flags')
social_dt<-social_dt %>% filter(spam ==0 & adult ==0)

print('Plotting the missing values and categorical columns')
plot_intro(social_dt)

numeric_columns<- dplyr::select_if(social_dt, is.numeric)
print('Plotting the distribution of variables')
plot_histogram(numeric_columns)
print('Plotting the correlation of items')
plot_correlation(na.omit(numeric_columns), maxcat = 5L)
GGally::ggpairs(social_dt, 
                columns = c('shopping', "travel", "politics", 'food', 'family'))

print('it seems that tv_films is correlated with school, beauty and art')
pca_df <- na.omit(social_dt[, c('shopping', "travel", "politics", 'food','school','beauty','art','school', 'family')])

print('Performaing a PCA analysis to understand the composition')
plot_prcomp(pca_df, variance_cap = 0.9, nrow = 2L, ncol = 2L)

print('Lets try to cluster the most correlated variables of this data')
numeric_columns<- dplyr::select_if(social_dt, is.numeric)
X = numeric_columns[c("politics","travel","school","beauty","art")]
X = scale(X, center=TRUE, scale=TRUE)

mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

clust1 = kmeans(X, 4, nstart=25)

clust1$center 

library(cluster)
library(fpc)
library(ppclust)
print('Lets look at the clusters for this set')
clusplot(X, clust1$cluster, color=TRUE, shade=TRUE,plotchar=TRUE)

print('Now lets try to combine PCA with Clustering for the most correlated variables')

library(ggfortify)
fit <- kmeans(X, 3, iter.max=1000) 

social_dt$segment<-fit$cluster

pca <- prcomp(X)
pca_data <- mutate(fortify(pca), col=fit$cluster) 

ggplot(pca_data) +  geom_point(aes(x=PC1, y=PC2, fill=factor(col)), 
                              size=3, col="#7f7f7f", shape=21) + theme_bw(base_family="Helvetica")

print('Number of members in each cluster')
barplot(table(fit$cluster), col="#336699")
print('Plotting the cluster')
autoplot(fit, data=X, frame=TRUE, frame.type='norm')
```

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

## Question 8 - The Reuters corpus

```{r}
library(tm) 
library(tidyverse)
library(slam)
library(proxy)
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
print('load data for all authors')
file_list = Sys.glob('ReutersC50/C50train/*/*.txt')
author = lapply(file_list, readerPlain) 
mynames = file_list %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist
names(author) = mynames

main_dir <- "ReutersC50/C50train/"
print ("list of directory names/Authors")
dir_list <- list.dirs(main_dir,full.names = FALSE, 
                      recursive = FALSE) 

library("corpus")
documents_raw = Corpus(VectorSource(author))
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) 
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) 
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) 
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) 

my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))

DTM_author = DocumentTermMatrix(my_documents)
class(DTM_author)  

DTM_author = removeSparseTerms(DTM_author, 0.95)
library(data.table) 

colS <- colSums(as.matrix(DTM_author))
length(colS)
doc_features <- data.table(name = attributes(colS)$names, count = colS)
print('most frequent and least frequent words across the entire dataset')
doc_features[order(-count)][1:10] #top 10 most frequent words
doc_features[order(count)][1:10] #least 10 frequent words
library(ggplot2)

library(ggthemes)
ggplot(doc_features[count>5000],aes(name, count))+
  geom_bar(stat = "identity",fill='lightblue',color='black')+
  labs(title="Most common words with freq greater than 5000 in entire dataset")

print('Word cloud of words with min frequency of 1000 in entire dataset')


library(wordcloud)
wordcloud(names(colS), colS, min.freq = 1000, scale = c(6,.1), colors = brewer.pal(6, 'Dark2'))

tfidf_author = weightTfIdf(DTM_author)
colS <- colSums(as.matrix(tfidf_author))
length(colS)
doc_features2 <- data.table(name = attributes(colS)$names, count = colS)
print('most frequent and least frequent words by number of documents')
doc_features2[order(-count)][1:10] #top 10 most frequent words
doc_features2[order(count)][1:10] #least 10 frequent words
ggplot(doc_features2[count>19],aes(name, count))+
  geom_bar(stat = "identity",fill='lightblue',color='black')+
  labs(title="Most common words with freq greater than 19 in individual document")
print('Word cloud of words with min frequency of 15 in individual dataset')

library(wordcloud)
wordcloud(names(colS), colS, min.freq = 15, scale = c(6,.1), colors = brewer.pal(6, 'Dark2'))
```

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

## Question 9 - Association rule mining

```{r}
library(tidyverse)
library(igraph)
library(arules)
library(arulesViz)

groceries_data = read.delim('/Users/akshayamudar/Desktop/groceries.txt', header = FALSE)
user_baskets = as.list(strsplit(groceries_data$V1, ","))
user_baskets = lapply(user_baskets, unique)

groceries_trans = as(user_baskets, "transactions")
summary(groceries_trans)
```

From the sparse matrix created using **arules** library we can see that the most frequent grocery items are *whole milk*, *other vegetables*, *rolls/buns*, *soda* and *yogurt*.

```{r}
grocery_con = apriori(groceries_trans, 
	parameter=list(support=.005, confidence=.1, maxlen=4))

arules::inspect(grocery_con)

plot(grocery_con)
```

```{r}
groceries_graph = associations2igraph(subset(grocery_con, lift>2), associationsAsNodes = FALSE)
plot(groceries_graph, edge.curved=FALSE)
igraph::write_graph(groceries_graph, file='groceries.graphml', format = "graphml")
```

## 
